# -*- coding: utf-8 -*-
"""water_quality.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lh9sKBlIYdWXjE5Z-M7RRjPVzf6IEz-y
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as ss
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import ADASYN, SMOTE
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV
import pickle
import warnings
warnings.filterwarnings ('ignore')
from scipy.stats import uniform

df=pd.read_csv('/content/water_potability.csv')

df.head()

df.shape

df.info()

df.isnull().sum()

from sklearn. impute import SimpleImputer

si=SimpleImputer (strategy="median")

df['ph']=si.fit_transform(df[['ph']])

df['Sulfate']=si.fit_transform(df[['Sulfate']])

df['Trihalomethanes']=si.fit_transform(df [[ 'Trihalomethanes']])

df.isnull().sum()

for i in df.columns:
  if type(df[i][0])!=str:
    sns.boxplot (df[i]) 
    plt.show()

quar = df.quantile(q = [0.25,0.75], axis=0)
quar

df['Turbidity']= np.where(df[ 'Turbidity']>6,4.500320,df[ 'Turbidity'])
df['ph']= np.where(df ['ph']>10,8.062066, df['ph'])
df['Hardness']= np. where (df['Hardness']>275,216.667456, df['Hardness'])
df['Solids']= np.where(df[ 'Solids' ]>44000, 27332.762127, df['Solids'])
df['Chloramines']= np.where(df['Chloramines']>11,6.127421, df[ 'Chloramines'])
df['Sulfate']= np.where(df[ 'Sulfate' ]>400,359.950178, df['Sulfate'])
df['Conductivity']= np.where(df[ 'Conductivity']>650,481.792304, df['Conductivity'])
df['Organic_carbon']= np.where(df['Organic_carbon' ]>23, 16.557652, df['Organic_carbon'])
df['Trihalomethanes']= np.where(df [ 'Trihalomethanes' ]>108,77.337473, df['Trihalomethanes'])

df['Turbidity']= np.where(df[ 'Turbidity']<2,55.844536, df[ 'Turbidity'])
df['ph']= np.where(df['ph']<4,6.093092, df ['ph'])
df[ 'Hardness']= np.where(df['Hardness'] <120,176.850538, df['Hardness'])
df['Chloramines']= np.where(df[ 'Chloramines']<3,6.127421,df['Chloramines'])
df['Sulfate']= np.where(df [ 'Sulfate']<260,307.699498, df[ 'Sulfate'])
df['Conductivity']= np.where(df [ 'Conductivity']<200,365.734414, df[ 'Conductivity'])
df['Organic_carbon']= np.where(df['Organic_carbon' ]<4,12.065801,df['Organic_carbon'])
df['Trihalomethanes']= np.where(df [ 'Trihalomethanes']<25,55.844536,df[ 'Trihalomethanes'])

data=df.copy()

features=data.drop('Potability', axis=1)
y_classlabel=data[['Potability']]

features.shape

y_classlabel.shape

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(features,y_classlabel, train_size=0.8, stratify=y_classlabel, random_state=100)

x_train.head(3)

df. Potability.value_counts()

from imblearn.over_sampling import ADASYN, SMOTE

ad=SMOTE (sampling_strategy=1)

x_train,y_train=ad.fit_resample (x_train,y_train)

pd.DataFrame(y_train).value_counts()

from sklearn.preprocessing import MinMaxScaler, StandardScaler

nrm=MinMaxScaler()
std=StandardScaler()

x_train['ph'] = std. fit_transform(x_train [['ph']]) 
x_train['Solids' ]=std. fit_transform(x_train [['Solids']])
x_train['Hardness']=std. fit_transform(x_train [['Hardness']])
x_train['Chloramines'] = std. fit_transform(x_train[['Chloramines']])
x_train['Sulfate']=std. fit_transform(x_train [['Sulfate']]) 
x_train['Conductivity' ]=std. fit_transform(x_train [ [ 'Conductivity']])
x_train['Organic_carbon' ]=std. fit_transform(x_train [['Organic_carbon']])
x_train['Turbidity']=std. fit_transform (x_train [ [ 'Turbidity']])
x_train['Trihalomethanes'] = std. fit_transform(x_train [ [ 'Trihalomethanes']])

x_test['ph'] = std. fit_transform(x_test[['ph']])
x_test['Hardness']=std. fit_transform(x_test[['Hardness']])
x_test['Solids'] = std. fit_transform(x_test[[ 'Solids']])
x_test['Chloramines']=std.fit_transform(x_test[['Chloramines']])
x_test['Sulfate']=std. fit_transform(x_test[['Sulfate']])
x_test['Conductivity'] = std. fit_transform(x_test [ [ 'Conductivity']])
x_test['Organic_carbon']=std. fit_transform(x_test[['Organic_carbon']])
x_test['Trihalomethanes'] = std. fit_transform(x_test [ [ 'Trihalomethanes']])
x_test['Turbidity' ]=std. fit_transform(x_test[['Turbidity']])

x_test

df.describe()

plt.figure(figsize=(5,5))
sns.distplot(df.ph)
plt.title("The Distribution of ph",
    loc='left',
    fontdict={'fontsize':20, 'color': 'orange'})
plt.grid()
plt.show()

plt.figure(figsize=(5,5))
sns.distplot(df. Hardness)
plt.title("The Distribution of Hardness", loc='left',
fontdict= {'fontsize' :20, 'color': 'orange'})
plt.grid()
plt.show()

plt.figure(figsize=(10,5))
sns.distplot(df. Solids, hist=True, color='red')
plt.title("The Distribution of Solids",
          loc='left',
          fontdict= {'fontsize' :20, 'color': 'orange'})
plt.show()

b=sns.countplot(x=df.Potability, data=df)
for container in b.containers:
      b.bar_label(container)
plt.title("The Count of Probability",
          loc='left', 
          fontdict= {'fontsize' :20, 'color': 'lime'})
plt.show()

sns.barplot(x=df.Trihalomethanes,y=df.Potability,data=df)
plt.title("The Portability vs Trihalomethanes",
    loc='left',
    fontdict= {'fontsize' :20, 'color': 'green'})
plt.show()

sns.violinplot (x=df. Solids,y=df.Potability) 
plt.title("The Portability vs Solids",
          loc='left',
          fontdict= {'fontsize' :20, 'color': 'blue'})
plt.show()

sns.boxplot (x=df. Trihalomethanes,y=df. Potability) 
plt.title("The distribution of Portability vs Trihalomethanes",
          loc='left',
          fontdict= {'fontsize' :20, 'color': 'blue'})
plt.show()

sns.pairplot(df,hue='Potability')
plt.show()

plt.figure(figsize=(15,10))
for i,col in enumerate (df.select_dtypes(exclude=['object']).columns,1):
  plt.subplot(4,3,i)
  plt.title(f"Distribution of {col}", fontdict= {'fontsize':20, 'color': 'indigo'})
  sns.histplot(df[col], kde=True)
  plt.tight_layout()
  plt.plot()

plt.figure(figsize=(10,5)) 
sns.heatmap(df.corr(), annot=True)
plt.show()

from sklearn.tree import DecisionTreeClassifier
dst=DecisionTreeClassifier(max_depth=445, max_features='log2')

model_4=dst.fit(x_train,y_train)
predict_decision=model_4.predict(x_test)
accuracy_score (y_test, predict_decision)

from sklearn.model_selection import cross_val_score
cv=cross_val_score(dst,x_train, y_train,cv=17)

np.mean(cv)

from sklearn.ensemble import RandomForestClassifier

rdf=RandomForestClassifier(n_estimators=30)

model_3=rdf.fit(x_train,y_train) 
predict_random=model_3.predict(x_test)
accuracy_score (y_test, predict_random)

cv=cross_val_score(rdf,x_train, y_train,cv=13)

np.mean(cv)

knn=KNeighborsClassifier (n_neighbors=18) 
model_1=knn.fit(x_train,y_train) 
predict_knn=model_1.predict(x_test)
accuracy_score (y_test, predict_knn)

print(classification_report (y_test, predict_knn))

print(confusion_matrix (y_test, predict_knn))

from xgboost import XGBClassifier
xgb=XGBClassifier()

model_6=dst.fit(x_train,y_train)
predict_xgb=model_6.predict(x_test) 
accuracy_score (y_test, predict_xgb)

def comparison(x_train,x_test,y_train,y_test):
    knn=KNeighborsClassifier()
    model_1=knn.fit(x_train,y_train) 
    predict_knn=model_1.predict(x_test)
    print("KNN Accuracy Score: ",accuracy_score (y_test, predict_knn))
    print("Classification Report: ")
    print(classification_report (y_test, predict_knn))
    print("Confusion Matrix: ")
    print(confusion_matrix(y_test, predict_knn)) 
    gnb=GaussianNB()
    model_2=gnb.fit(x_train,y_train)
    predict_log=model_2.predict(x_test)
    print("Naive Bayes Accuracy Score: ",accuracy_score (y_test, predict_log))
    print("classification Report: ") 
    print(classification_report (y_test, predict_log))
    print("Confusion Matrix: ") 
    print(confusion_matrix(y_test, predict_log))
    rdf=RandomForestClassifier() 
    model_3=rdf.fit(x_train,y_train)
    predict_random=model_3.predict(x_test)
    print("Random Forest Accuracy Score: ",accuracy_score (y_test, predict_random))
    print("Classification Report: ") 
    print(classification_report (y_test, predict_random))
    print("Confusion Matrix: ")
    print(confusion_matrix(y_test, predict_random))
    dst=DecisionTreeClassifier(max_depth=445, max_features="log2") 
    model_4=dst.fit(x_train,y_train)
    predict_decision=model_4.predict(x_test) 
    print("Decision Tree Accuracy Score: ",accuracy_score (y_test, predict_decision))
    print("Classification Report: ")
    print(classification_report (y_test, predict_decision))
    print("Confusion Matrix: ")
    print(confusion_matrix (y_test, predict_decision))
    xgb=XGBClassifier()
    model_6=dst.fit(x_train,y_train)
    predict_xgb=model_6=dst.predict(x_test)
    print("XGBoost Accuracy Score: ",accuracy_score (y_test, predict_xgb))
    print("Classification Report: ") 
    print(classification_report (y_test, predict_xgb))

comparison(x_train,x_test,y_train,y_test)

from sklearn.model_selection import RandomizedSearchCV 
from sklearn.model_selection import GridSearchCV

n_estimators = [int(x) for x in np.linspace(start = 100, stop = 200, num = 10)]
max_features = ['auto', 'sqrt', 'log2'] 
max_depth = [int(x) for x in np.linspace (10, 500,10)] 
min_samples_split = [2, 5, 10,14]
min_samples_leaf = [1, 2, 4,6,8]  
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features, 
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min samples leaf': min_samples_leaf,
               'criterion': ['entropy', 'gini']}

print(random_grid)

rf=RandomForestClassifier()
rf_randomcv=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100, cv=3)

comparison (x_train,x_test,y_train,y_test)

from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV

n_estimators = [int(x) for x in np.linspace (start=100, stop=200,num =10)]
max_features= ['auto', 'sqrt', 'log2']
max_depth = [int(x) for x in np.linspace (10, 500,10)]
min_samples_split = [2, 5, 10,14]
min_samples_leaf = [1, 2, 4,6,8]
random_grid ={'n_estimators': n_estimators,
              'max_features': max_features,
              'max_depth': max_depth, 
              'min_samples_leaf': min_samples_leaf,
              'min_samples_split': min_samples_split,
              'criterion': ['entropy', 'gini']}
print (random_grid)

rf=RandomForestClassifier()
rf_randomcv=RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=100, cv=3)

rf_randomcv.fit(x_train,y_train)

y_pred=rf_randomcv.predict(x_test) 
print(confusion_matrix (y_test,y_pred))
print("Accuracy Score {}".format (accuracy_score (y_test,y_pred)))
print("Classification report: {}".format(classification_report (y_test,y_pred)))